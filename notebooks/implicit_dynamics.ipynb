{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact solutions, decoupling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "import pathlib\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "import sys\n",
    "sys.path.append('../code/')\n",
    "from linear_utils import linear_model, get_modulation_matrix\n",
    "from train_utils import save_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 20\n",
    "n = 1000\n",
    "r = 5\n",
    "\n",
    "B = np.random.uniform(low=0, high=1, size=(d, r))\n",
    "D = np.diag(np.array([4, 2, 1, 1/2, 1/4]))\n",
    "Z = np.random.multivariate_normal(mean=np.zeros(r,), cov=D, size=(n,))\n",
    "eps = 10**(-3) * np.random.standard_normal(size=(n, d))\n",
    "\n",
    "X = Z@B.T + eps\n",
    "\n",
    "beta = B@D@B.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.tensor(X, dtype=torch.float32)\n",
    "ys = torch.tensor(X, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "risk_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "           torch.nn.Linear(d, d, bias=False),\n",
    "         )      \n",
    "                \n",
    "stepsize = 0.000001\n",
    "iterations = 100000\n",
    "print_freq = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "losses_emp = []\n",
    "mse_weights_emp = []\n",
    "ws = []\n",
    "\n",
    "\n",
    "for t in range(int(iterations)):\n",
    "    \n",
    "    y_pred = model(Xs)\n",
    "\n",
    "    loss = loss_fn(y_pred, ys)\n",
    "    losses_emp.append(loss.item())\n",
    "\n",
    "    if not t % print_freq:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        w_tot = torch.diag(torch.ones(d)) #[]\n",
    "        for param in model.parameters():\n",
    "                \n",
    "            param.data -= stepsize * param.grad\n",
    "                                \n",
    "            w_tot = w_tot @ param.data.t()\n",
    "            if len(param.shape) > 1:\n",
    "                i += 1\n",
    "        \n",
    "        w_tot = w_tot.squeeze()\n",
    "        assert w_tot.shape == beta.shape\n",
    "\n",
    "        ws.append(w_tot)\n",
    "        mse_weights_emp.append(((w_tot-beta.squeeze()) / beta.squeeze())**2) #w_tot\n",
    "                \n",
    "\n",
    "losses_1 = np.array(losses_emp)\n",
    "risks_w_1 = np.row_stack([mse_w.reshape(-1) for mse_w in mse_weights_emp])\n",
    "w_norm_1 = np.row_stack([np.linalg.norm(w) for w in ws])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "           torch.nn.Linear(d, r, bias=False),\n",
    "           torch.nn.Linear(r, d, bias=False),\n",
    "         )      \n",
    "                \n",
    "stepsize = 0.000001\n",
    "iterations = 100000\n",
    "print_freq = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "losses_emp = []\n",
    "mse_weights_emp = []\n",
    "ws = []\n",
    "\n",
    "\n",
    "for t in range(int(iterations)):\n",
    "    \n",
    "    y_pred = model(Xs)\n",
    "\n",
    "    loss = loss_fn(y_pred, ys)\n",
    "    losses_emp.append(loss.item())\n",
    "\n",
    "    if not t % print_freq:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        w_tot = torch.diag(torch.ones(d)) #[]\n",
    "        for param in model.parameters():\n",
    "                \n",
    "            param.data -= stepsize * param.grad\n",
    "                                \n",
    "            w_tot = w_tot @ param.data.t()\n",
    "            if len(param.shape) > 1:\n",
    "                i += 1\n",
    "        \n",
    "        w_tot = w_tot.squeeze()\n",
    "        assert w_tot.shape == beta.shape\n",
    "\n",
    "        ws.append(w_tot)\n",
    "        mse_weights_emp.append(((w_tot-beta.squeeze()) / beta.squeeze())**2) #w_tot\n",
    "                \n",
    "\n",
    "losses_2 = np.array(losses_emp)\n",
    "risks_w_2 = np.row_stack([mse_w.reshape(-1) for mse_w in mse_weights_emp])\n",
    "w_norm_2 = np.row_stack([np.linalg.norm(w) for w in ws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_samples = [int(i) for i in np.geomspace(1, len(losses_emp)-1, num=700)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "colorList = [cmap(50/1000), cmap(350/1000), cmap(700/1000)]\n",
    "labelList = ['One layer', 'Two layers']\n",
    "\n",
    "plot_all_dims = True\n",
    "\n",
    "num_axs = 3 + 5 if plot_all_dims else 3\n",
    "\n",
    "fig, ax = plt.subplots(num_axs, 1, figsize=(12, 4 * num_axs))\n",
    "\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].plot(geo_samples, w_norm_2[geo_samples], \n",
    "        color=colorList[1], \n",
    "        label=labelList[1],\n",
    "        lw=4)\n",
    "\n",
    "ax[0].plot(geo_samples, w_norm_1[geo_samples], \n",
    "        color=colorList[0], \n",
    "        label=labelList[0],\n",
    "        lw=4)\n",
    "\n",
    "\n",
    "ax[0].legend(loc=1, bbox_to_anchor=(1, 1), fontsize='x-large',\n",
    "    frameon=False, fancybox=True, shadow=True, ncol=1)\n",
    "ax[0].set_ylabel('norm')\n",
    "ax[0].set_xlabel(r'$t$ iterations')\n",
    "\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].plot(geo_samples, losses_2[geo_samples], \n",
    "        color=colorList[1], \n",
    "        label=labelList[1],\n",
    "        lw=4)\n",
    "\n",
    "ax[1].plot(geo_samples, losses_1[geo_samples], \n",
    "        color=colorList[0], \n",
    "        label=labelList[0],\n",
    "        lw=4)\n",
    "\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel(r'$t$ iterations')\n",
    "\n",
    "\n",
    "if plot_all_dims:\n",
    "    for i in range(5): #range(risks_w_2.shape[-1]):\n",
    "        ax[2+i].set_xscale('log')\n",
    "        ax[2+i].plot(geo_samples, risks_w_2[geo_samples, i], \n",
    "                color=colorList[1], \n",
    "                label=labelList[1],\n",
    "                lw=4)\n",
    "        \n",
    "        ax[2+i].plot(geo_samples, risks_w_1[geo_samples, i], \n",
    "        color=colorList[0], \n",
    "        label=labelList[0],\n",
    "        lw=4)\n",
    "\n",
    "        ax[2+i].set_ylabel('MSE weights, ' + str(i))\n",
    "        ax[2+i].set_xlabel(r'$t$ iterations')\n",
    "\n",
    "\n",
    "ax[-1].set_xscale('log')\n",
    "ax[-1].plot(geo_samples, risks_w_2[geo_samples, :].mean(axis=-1), \n",
    "        color=colorList[1], \n",
    "        label=labelList[1],\n",
    "        lw=4)\n",
    "\n",
    "ax[-1].plot(geo_samples, risks_w_1[geo_samples, :].mean(axis=-1), \n",
    "        color=colorList[0], \n",
    "        label=labelList[0],\n",
    "        lw=4)\n",
    "\n",
    "\n",
    "ax[-1].set_ylabel('MSE weights')\n",
    "ax[-1].set_xlabel(r'$t$ iterations')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Har detta något att göra med att här har sanna beta en lägre rang? Och enlagermodellen hittar inte denna matris av lägre rang...\n",
    "# Men är det att enlagerdynamiken rör sig i ett annat rum?\n",
    "\n",
    "print(beta.shape)\n",
    "print(np.linalg.matrix_rank(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = np.linalg.svd(Xs.T@Xs)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
