{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact solutions, decoupling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "import pathlib\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "import sys\n",
    "sys.path.append('../code/')\n",
    "from linear_utils import linear_model, get_modulation_matrix\n",
    "from train_utils import save_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument written in command line format\n",
    "cli_args = '--seed 12 --save-results --risk-loss L2 -t 100000 -w 0.1 0.1 --lr 0.001 0.001 -d 2 -n 50 --hidden 50 --sigmas 1 --kappa 10.0'\n",
    "sigma_noise = 0.0\n",
    "transform_data = True\n",
    "cont_eigs = False\n",
    "\n",
    "#cli_args = '--seed 12 --save-results --jacobian --risk-loss L2 -t 20000 -w 0.1 0.1 --lr 0.00001 -d 50 -n 1000 --hidden 50 --sigmas 1 --kappa 3'\n",
    "#sigma_noise = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get CLI parameters\n",
    "parser = argparse.ArgumentParser(description='CLI parameters for training')\n",
    "parser.add_argument('--root', type=str, default='', metavar='DIR',\n",
    "                    help='Root directory')\n",
    "parser.add_argument('-t', '--iterations', type=int, default=1e4, metavar='ITERATIONS',\n",
    "                    help='Iterations (default: 1e4)')\n",
    "parser.add_argument('-n', '--samples', type=int, default=100, metavar='N',\n",
    "                    help='Number of samples (default: 100)')\n",
    "parser.add_argument('--print-freq', type=int, default=1000,\n",
    "                    help='CLI output printing frequency (default: 1000)')\n",
    "parser.add_argument('--gpu', type=int, default=None,\n",
    "                    help='Number of GPUS to use')\n",
    "parser.add_argument('--seed', type=int, default=None,\n",
    "                    help='Random seed')                        \n",
    "parser.add_argument('-d', '--dim', type=int, default=50, metavar='DIMENSION',\n",
    "                    help='Feature dimension (default: 50)')\n",
    "parser.add_argument('--hidden', type=int, default=200, metavar='DIMENSION',\n",
    "                    help='Hidden layer dimension (default: 200)')\n",
    "parser.add_argument('--sigmas', type=str, default=None,\n",
    "                    help='Sigmas')     \n",
    "parser.add_argument('-r','--s-range', nargs='*', type=float,\n",
    "                    help='Range for sigmas')\n",
    "parser.add_argument('--kappa', type=float,\n",
    "                    help='Eigenvalue ratio')\n",
    "parser.add_argument('-w','--scales', nargs='*', type=float,\n",
    "                    help='scale of the weights')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, nargs='*', metavar='LR',\n",
    "                    help='learning rate (default: 1e-4)')              \n",
    "parser.add_argument('--normalized', action='store_true', default=False,\n",
    "                    help='normalize sample norm across features')\n",
    "parser.add_argument('--risk-loss', type=str, default='MSE', metavar='LOSS',\n",
    "                    help='Loss for validation')\n",
    "parser.add_argument('--jacobian', action='store_true', default=False,\n",
    "                    help='compute the SVD of the jacobian of the network')\n",
    "parser.add_argument('--save-results', action='store_true', default=False,\n",
    "                    help='Save the results for plots')\n",
    "parser.add_argument('--details', type=str, metavar='N',\n",
    "                    default='no_detail_given',\n",
    "                    help='details about the experimental setup')\n",
    "\n",
    "\n",
    "args = parser.parse_args(cli_args.split())\n",
    "\n",
    "# directories\n",
    "root = pathlib.Path(args.root) if args.root else pathlib.Path.cwd().parent\n",
    "\n",
    "current_date = str(datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "args.outpath = (pathlib.Path.cwd().parent / 'results' / 'two_layer_nn' /  current_date)\n",
    "\n",
    "if args.save_results:\n",
    "    args.outpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "if args.seed is not None:\n",
    "    random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_eigs = 0 if args.samples >= args.dim else (args.dim - args.samples)\n",
    "p = int(np.ceil((args.dim - zero_eigs) / 2))\n",
    "#k = args.kappa\n",
    "#F = get_modulation_matrix(args.dim, p, k)\n",
    "\n",
    "\n",
    "d_out = 2      # dimension of y\n",
    "\n",
    "beta = np.ones((args.dim, d_out))\n",
    "\n",
    "# Set to False if you want to transform after data sampling\n",
    "scale_beta = False\n",
    "\n",
    "# sample training set from the linear model\n",
    "lin_model = linear_model(args.dim, dy=d_out, sigma_noise=sigma_noise, beta=beta, scale_beta=scale_beta, normalized=False, sigmas=args.sigmas, s_range=args.s_range, coupled_noise=False, transform_data=transform_data, kappa=args.kappa, p=p, cont_eigs=cont_eigs, zero_eigs=zero_eigs)\n",
    "Xs, ys = lin_model.sample(args.samples, train=True)\n",
    "\n",
    "# sample the set for empirical risk calculation\n",
    "Xt, yt = lin_model.sample(args.samples * 100, train=False) # 1000\n",
    "beta = lin_model.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.zeros((d_out, args.dim))\n",
    "beta[0, 0] = 0.01001\n",
    "beta[1, 1] = 1\n",
    "ys = Xs @ beta.T\n",
    "yt = Xt @ beta.T\n",
    "\n",
    "beta = beta.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.tensor(Xs, dtype=torch.float32).to(device)\n",
    "ys = torch.tensor(ys.reshape((-1, d_out)), dtype=torch.float32).to(device)\n",
    "\n",
    "Xt = torch.tensor(Xt, dtype=torch.float32).to(device)\n",
    "yt = torch.tensor(yt.reshape((-1, d_out)), dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.svd(ys.T @ Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, Vhyx = np.linalg.svd(ys.T @ Xs)\n",
    "Vhyx = torch.tensor(Vhyx)\n",
    "np.linalg.svd(Vhyx @ Xs.T @ Xs @ Vhxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "risk_fn = torch.nn.L1Loss(reduction='sum') if args.risk_loss == 'L1' else torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical, diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal_init(model, X, y, g_cpu, args, synaptic=False):\n",
    "    \n",
    "    Uxy, _, Vhxy = np.linalg.svd(y.T @ X @ torch.inverse(X.T @ X))\n",
    "    \n",
    "    if args.hidden > 1:\n",
    "        R = torch.tensor(ortho_group.rvs(dim=args.hidden), dtype=torch.float32)\n",
    "    else:\n",
    "        R = torch.tensor([[1]], dtype=torch.float32)\n",
    "    \n",
    "    i = 0\n",
    "    w_check = None\n",
    "    with torch.no_grad(): \n",
    "        p, q, u = 0, 0, 0\n",
    "        for m in model:\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                     \n",
    "                \n",
    "                if i == 0:\n",
    "                    D = torch.zeros(m.weight.data.shape, dtype=torch.float32).fill_diagonal_(1, wrap=False)\n",
    "                    D[D == 1] = torch.normal(mean=0, std=args.scales[0], size=(min(m.weight.data.shape[0], m.weight.data.shape[1]),), generator=g_cpu)\n",
    "                    \n",
    "                    \n",
    "                    if synaptic:\n",
    "                        m.weight.data = torch.matmul(R, D)\n",
    "                    else:\n",
    "                        m.weight.data = torch.matmul(torch.matmul(R, D), torch.tensor(Vhxy))\n",
    "                    \n",
    "                    w_check = m.weight.data.clone()\n",
    "                     \n",
    "                else:\n",
    "                    \n",
    "                    if i == 1:\n",
    "                        D = torch.zeros(m.weight.data.shape, dtype=torch.float32).fill_diagonal_(1, wrap=False)\n",
    "                        D[D == 1] = torch.normal(mean=0, std=args.scales[1], size=(min(m.weight.data.shape[0], m.weight.data.shape[1]),), generator=g_cpu)\n",
    "                        \n",
    "                        if synaptic: \n",
    "                            m.weight.data = torch.matmul(D, R.T)\n",
    "                        else:\n",
    "                            m.weight.data = torch.matmul(torch.tensor(Uxy), torch.matmul(D, R.T))\n",
    "                        \n",
    "                        w_check = m.weight.data.clone() @ w_check\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"Initialisation only supported for two layers.\")\n",
    "               \n",
    "                i += 1\n",
    "   \n",
    "        print(\"Initial weight: \\n {}\".format(w_check))\n",
    "        w_check = (torch.tensor(Uxy).T @ w_check @ torch.tensor(Vhxy).T).numpy()\n",
    "        print(\"Transformed initial weight: \\n {}\".format(w_check))\n",
    "\n",
    "        #assert np.count_nonzero(w_check - np.diag(np.diagonal(w_check))) == 0\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "           torch.nn.Linear(args.dim, args.hidden, bias=False),\n",
    "           #torch.nn.ReLU(),\n",
    "           torch.nn.Linear(args.hidden, d_out, bias=False),\n",
    "         ).to(device)      \n",
    "                \n",
    "# initialization \n",
    "synaptic = True\n",
    "manual_grads = False\n",
    "decaying_lr = False\n",
    "\n",
    "XX = Xs.T @ Xs\n",
    "yX = ys.T @ Xs\n",
    "Uxy, Sxy, Vhxy = np.linalg.svd(yX @ torch.inverse(XX))\n",
    "Uxy, Sxy, Vhxy = torch.tensor(Uxy), torch.diag(torch.tensor(Sxy)), torch.tensor(Vhxy)\n",
    "VXXV = Vhxy.T @ Xs.T @ Xs @ Vhxy\n",
    "\n",
    "g_cpu = torch.Generator()\n",
    "g_cpu.manual_seed(args.seed)\n",
    "model = diagonal_init(model, Xs, ys, g_cpu, args, synaptic=synaptic)\n",
    " \n",
    "\n",
    "# use same learning rate for the two layers\n",
    "#if isinstance(args.lr, list):\n",
    "#    stepsize = [max(args.lr)] * 2\n",
    "stepsize = args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wtot = torch.diag(torch.ones(args.dim))\n",
    "for param in model.parameters():\n",
    "    if len(param.shape) > 1:\n",
    "        Wtot = Wtot @ param.data.t()\n",
    "        \n",
    "print(Wtot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "losses_emp = []\n",
    "risks_emp = []\n",
    "mse_weights_emp = []\n",
    "ws = []\n",
    "\n",
    "base_stepsize = stepsize.copy()\n",
    "print(stepsize[0])\n",
    "gamma = 0.0001\n",
    "for t in range(int(args.iterations)):\n",
    "    \n",
    "    if synaptic:\n",
    "        y_pred = Xs @ torch.matmul(Uxy, torch.matmul(torch.matmul(model[1].weight, model[0].weight),  Vhxy)).T\n",
    "    else:\n",
    "        y_pred = model(Xs)\n",
    "   \n",
    "\n",
    "    loss = loss_fn(y_pred, ys)\n",
    "    losses_emp.append(loss.item())\n",
    "\n",
    "    if not t % args.print_freq:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        w_tot = torch.diag(torch.ones(args.dim)) #[]\n",
    "        for param in model.parameters():\n",
    "                \n",
    "                \n",
    "            if synaptic or manual_grads:\n",
    "                \n",
    "                if synaptic: \n",
    "                    grad_diff = torch.matmul(Sxy - torch.matmul(model[1].weight, model[0].weight), VXXV)\n",
    "                elif manual_grads:\n",
    "                    grad_diff = yX - torch.matmul(torch.matmul(model[1].weight, model[0].weight), XX)\n",
    "\n",
    "                if i == 0:\n",
    "                    grad = - torch.matmul(model[1].weight.T, grad_diff)\n",
    "                    param.data -= stepsize[i] * grad\n",
    "                elif i == 1:\n",
    "                    grad = - torch.matmul(grad_diff, model[0].weight.T)\n",
    "                    param.data -= stepsize[i] * grad\n",
    "\n",
    "                else:\n",
    "                    print(\"Training in synaptic weight space and manual gradients only supported for two layers.\")\n",
    "            else:\n",
    "                param.data -= stepsize[i] * param.grad\n",
    "                                \n",
    "            w_tot = w_tot @ param.data.t()\n",
    "            if len(param.shape) > 1:\n",
    "                i += 1\n",
    "        \n",
    "        w_tot = w_tot.squeeze()\n",
    "        ws.append(w_tot)\n",
    "        assert w_tot.shape == beta.squeeze().shape\n",
    "        mse_weights_emp.append((w_tot-beta.squeeze())**2)#((w_tot-beta.squeeze()) / beta.squeeze())**2) #w_tot\n",
    "                \n",
    "                    \n",
    "        if synaptic:\n",
    "            yt_pred = Xt @ torch.matmul(Uxy, torch.matmul(torch.matmul(model[1].weight, model[0].weight), Vhxy)).T\n",
    "        else:\n",
    "            yt_pred = model(Xt)\n",
    "\n",
    "            \n",
    "        risk = risk_fn(yt_pred, yt)\n",
    "        risks_emp.append(risk.item())\n",
    "\n",
    "        if not t % args.print_freq:\n",
    "            print(t, risk.item())\n",
    "            \n",
    "    if decaying_lr:\n",
    "        stepsize = [base_lr / (1 + gamma * (t+1)) for base_lr in base_stepsize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_samples = [int(i) for i in np.geomspace(1, len(risks_emp)-1, num=700)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse_weights_emp[-1])\n",
    "print(mse_weights_emp[-1].reshape(-1))\n",
    "print(w_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = np.array(risks_emp)\n",
    "losses = np.array(losses_emp)\n",
    "risks_w = np.row_stack([mse_w.reshape(-1) for mse_w in mse_weights_emp])\n",
    "\n",
    "#Uyx, _, Vhyx = np.linalg.svd(ys.T @ Xs)\n",
    "#Uyx, Vhyx = torch.tensor(Uyx, dtype=torch.float32), torch.tensor(Vhyx, dtype=torch.float32)\n",
    "#ws_transf = [(Uyx @ w.T @ Vhyx).T for w in ws]\n",
    "#risks_w_transf = np.row_stack([((w-beta)**2).reshape(-1) for w in ws_transf])\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "colorList = [cmap(50/1000), cmap(350/1000), cmap(700/1000)]\n",
    "labelList = ['empirical', 'theoretical']\n",
    "\n",
    "plot_all_dims = True\n",
    "\n",
    "num_axs = 3 + risks_w.shape[-1] if plot_all_dims else 3\n",
    "\n",
    "fig, ax = plt.subplots(num_axs, 1, figsize=(12, 4 * num_axs))\n",
    "\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].plot(geo_samples, risks[geo_samples], \n",
    "        color=colorList[1], \n",
    "        label=labelList[0],\n",
    "        lw=4)\n",
    "\n",
    "ax[0].legend(loc=1, bbox_to_anchor=(1, 1), fontsize='x-large',\n",
    "    frameon=False, fancybox=True, shadow=True, ncol=1)\n",
    "ax[0].set_ylabel('risk')\n",
    "ax[0].set_xlabel(r'$t$ iterations')\n",
    "\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].plot(geo_samples, losses[geo_samples], \n",
    "        color=colorList[1], \n",
    "        label=labelList[0],\n",
    "        lw=4)\n",
    "\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel(r'$t$ iterations')\n",
    "\n",
    "\n",
    "if plot_all_dims:\n",
    "    for i in range(risks_w.shape[-1]):\n",
    "        ax[2+i].set_xscale('log')\n",
    "        ax[2+i].plot(geo_samples, risks_w[geo_samples, i], \n",
    "                color=colorList[2], \n",
    "                label=labelList[0],\n",
    "                lw=4)\n",
    "\n",
    "        ax[2+i].set_ylabel('MSE weights, ' + str(i))\n",
    "        ax[2+i].set_xlabel(r'$t$ iterations')\n",
    "\n",
    "\n",
    "ax[-1].set_xscale('log')\n",
    "ax[-1].plot(geo_samples, risks_w[geo_samples, :].mean(axis=-1), \n",
    "        color=colorList[2], \n",
    "        label=labelList[0],\n",
    "        lw=4)\n",
    "\n",
    "ax[-1].set_ylabel('MSE weights')\n",
    "ax[-1].set_xlabel(r'$t$ iterations')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYNAPTIC WEIGHT SPACE\n",
    "\n",
    "if synaptic: \n",
    "    track_w = np.row_stack([w.T.reshape(-1) for w in ws]) # (Uxy @ w.T @ Vhxy) \n",
    "else:\n",
    "    track_w = np.row_stack([(Uxy.T @ w.T.reshape(d_out, -1) @ Vhxy.T).reshape(-1) for w in ws]) \n",
    "    \n",
    "fig, ax = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "ax[0, 0].plot(track_w[:, 0], track_w[:, 1])\n",
    "ax[0, 0].plot(track_w[0, 0], track_w[0, 1], '*')\n",
    "ax[0, 0].set_xlabel(\"Weight 1\")\n",
    "ax[0, 0].set_ylabel(\"Weight 2\")\n",
    "\n",
    "ax[0, 1].plot(track_w[:, 0], track_w[:, 2])\n",
    "ax[0, 1].plot(track_w[0, 0], track_w[0, 2], '*')\n",
    "ax[0, 1].set_xlabel(\"Weight 1\")\n",
    "ax[0, 1].set_ylabel(\"Weight 3\")\n",
    "\n",
    "ax[0, 2].plot(track_w[:, 0], track_w[:, 3])\n",
    "ax[0, 2].plot(track_w[0, 0], track_w[0, 3], '*')\n",
    "ax[0, 2].set_xlabel(\"Weight 1\")\n",
    "ax[0, 2].set_ylabel(\"Weight 4\")\n",
    "\n",
    "ax[1, 0].plot(track_w[:, 1], track_w[:, 2])\n",
    "ax[1, 0].plot(track_w[0, 1], track_w[0, 2], '*')\n",
    "ax[1, 0].set_xlabel(\"Weight 2\")\n",
    "ax[1, 0].set_ylabel(\"Weight 3\")\n",
    "\n",
    "ax[1, 1].plot(track_w[:, 1], track_w[:, 3])\n",
    "ax[1, 1].plot(track_w[0, 1], track_w[0, 3], '*')\n",
    "ax[1, 1].set_xlabel(\"Weight 2\")\n",
    "ax[1, 1].set_ylabel(\"Weight 4\")\n",
    "\n",
    "ax[1, 2].plot(track_w[:, 2], track_w[:, 3])\n",
    "ax[1, 2].plot(track_w[0, 2], track_w[0, 3], '*')\n",
    "ax[1, 2].set_xlabel(\"Weight 3\")\n",
    "ax[1, 2].set_ylabel(\"Weight 4\")\n",
    "\n",
    "#for axis in ax.reshape(-1):\n",
    "#    axis.plot([0, 1], [0, 1])\n",
    "#    axis.plot\n",
    "\n",
    "_, S, Vh = np.linalg.svd((Uxy.T @ XX @ Vhxy.T).numpy())\n",
    "print(S)\n",
    "for axis in ax.reshape(-1):\n",
    "    axis.arrow(0, 0, Vh[0, 0], Vh[0, 1])\n",
    "    axis.annotate(\"\", xy=(Vh[0, 0], Vh[0, 1]), xytext=(0, 0), \n",
    "                      arrowprops=dict(arrowstyle=\"->\"))\n",
    "    axis.arrow(0, 0, Vh[1, 0], Vh[1, 1])\n",
    "    axis.annotate(\"\", xy=(Vh[1, 0], Vh[1, 1]), xytext=(0, 0), \n",
    "                  arrowprops=dict(arrowstyle=\"->\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL WEIGHT SPACE\n",
    "\n",
    "if synaptic: \n",
    "    track_w = np.row_stack([(Uxy @ w.T.reshape(d_out, -1) @ Vhxy).reshape(-1) for w in ws]) # \n",
    "else:\n",
    "    track_w = np.row_stack([w.T.reshape(-1) for w in ws]) \n",
    "    \n",
    "fig, ax = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "ax[0, 0].plot(track_w[:, 0], track_w[:, 1])\n",
    "ax[0, 0].plot(track_w[0, 0], track_w[0, 1], '*')\n",
    "ax[0, 0].set_xlabel(\"Weight 1\")\n",
    "ax[0, 0].set_ylabel(\"Weight 2\")\n",
    "\n",
    "ax[0, 1].plot(track_w[:, 0], track_w[:, 2])\n",
    "ax[0, 1].plot(track_w[0, 0], track_w[0, 2], '*')\n",
    "ax[0, 1].set_xlabel(\"Weight 1\")\n",
    "ax[0, 1].set_ylabel(\"Weight 3\")\n",
    "\n",
    "ax[0, 2].plot(track_w[:, 0], track_w[:, 3])\n",
    "ax[0, 2].plot(track_w[0, 0], track_w[0, 3], '*')\n",
    "ax[0, 2].set_xlabel(\"Weight 1\")\n",
    "ax[0, 2].set_ylabel(\"Weight 4\")\n",
    "\n",
    "ax[1, 0].plot(track_w[:, 1], track_w[:, 2])\n",
    "ax[1, 0].plot(track_w[0, 1], track_w[0, 2], '*')\n",
    "ax[1, 0].set_xlabel(\"Weight 2\")\n",
    "ax[1, 0].set_ylabel(\"Weight 3\")\n",
    "\n",
    "ax[1, 1].plot(track_w[:, 1], track_w[:, 3])\n",
    "ax[1, 1].plot(track_w[0, 1], track_w[0, 3], '*')\n",
    "ax[1, 1].set_xlabel(\"Weight 2\")\n",
    "ax[1, 1].set_ylabel(\"Weight 4\")\n",
    "\n",
    "ax[1, 2].plot(track_w[:, 2], track_w[:, 3])\n",
    "ax[1, 2].plot(track_w[0, 2], track_w[0, 3], '*')\n",
    "ax[1, 2].set_xlabel(\"Weight 3\")\n",
    "ax[1, 2].set_ylabel(\"Weight 4\")\n",
    "\n",
    "#for axis in ax.reshape(-1):\n",
    "#    axis.plot([0, 1], [0, 1])\n",
    "#    axis.plot\n",
    "\n",
    "\n",
    "for axis in ax.reshape(-1):\n",
    "    axis.arrow(0, 0, Vhxy[0, 0], Vhxy[0, 1])\n",
    "    axis.annotate(\"\", xy=(Vhxy[0, 0], Vhxy[0, 1]), xytext=(0, 0), \n",
    "                      arrowprops=dict(arrowstyle=\"->\"))\n",
    "    axis.arrow(0, 0, Vhxy[1, 0], Vhxy[1, 1])\n",
    "    axis.annotate(\"\", xy=(Vhxy[1, 0], Vhxy[1, 1]), xytext=(0, 0), \n",
    "                  arrowprops=dict(arrowstyle=\"->\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final weights:\")\n",
    "print(ws[-1].T) \n",
    "\n",
    "if synaptic:\n",
    "    print(f\"Global minimum: \\n {Sxy}\")\n",
    "    print(f\"Transformed final weights (true space): \\n {Uxy@ws[-1].T@Vhxy}\")\n",
    "    print(f\"Transformed global minimum (true space): \\n {ys.T @ Xs @ torch.inverse(Xs.T @ Xs)}\")\n",
    "    print(f\"Final loss: {loss_fn(Xs @ (Uxy@ws[-1].T@Vhxy).T, ys)}\")\n",
    "else:\n",
    "    print(f\"Global minimum: \\n {ys.T @ Xs @ torch.inverse(Xs.T @ Xs)}\")\n",
    "    print(f\"Transformed final weights (synaptic weight space): \\n {Uxy.T@ws[-1].T@Vhxy.T}\")\n",
    "    print(f\"Transformed global minimum (synaptic weight space): \\n {Sxy}\")\n",
    "    print(f\"Final loss: \\n {loss_fn(Xs @ ws[-1], ys)}\")\n",
    "\n",
    "print(f\"Loss of global minimum: \\n {loss_fn(Xs @ (ys.T @ Xs @ torch.inverse(Xs.T @ Xs)).T, ys)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
