{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Double descent in two layer neural network\n",
    "From \"Early stopping in deep neural networks: double descent and how to eliminate it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "import pathlib\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../code/')\n",
    "from linear_utils import linear_model\n",
    "from train_utils import save_config, prune_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument written in command line format\n",
    "cli_args = '--seed 12 --save-results --jacobian --risk-loss L1 -t 50000 -w 1 0.1 --lr 0.0001 0.0001 -d 100 -n 100 --hidden 50 --sigmas 1 --coupled_noise --sigma_noise 10.0 2.0 --no-bias --linear --transform-data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A fully-connected ReLU network with one hidden layer, trained to predict y from x\n",
    "by minimizing the MSE loss.\n",
    "\"\"\"\n",
    "\n",
    "# get CLI parameters\n",
    "parser = argparse.ArgumentParser(description='CLI parameters for training')\n",
    "parser.add_argument('--root', type=str, default='', metavar='DIR',\n",
    "                    help='Root directory')\n",
    "parser.add_argument('-t', '--iterations', type=int, default=1e4, metavar='ITERATIONS',\n",
    "                    help='Iterations (default: 1e4)')\n",
    "parser.add_argument('-n', '--samples', type=int, default=100, metavar='N',\n",
    "                    help='Number of samples (default: 100)')\n",
    "parser.add_argument('--print-freq', type=int, default=100,\n",
    "                    help='CLI output printing frequency (default: 1000)')\n",
    "parser.add_argument('--gpu', type=int, default=None,\n",
    "                    help='Number of GPUS to use')\n",
    "parser.add_argument('--seed', type=int, default=None,\n",
    "                    help='Random seed')                        \n",
    "parser.add_argument('-d', '--dim', type=int, default=50, metavar='DIMENSION',\n",
    "                    help='Feature dimension (default: 50)')\n",
    "parser.add_argument('--hidden', type=int, default=200, metavar='DIMENSION',\n",
    "                    help='Hidden layer dimension (default: 200)')\n",
    "parser.add_argument('--no-bias', action='store_true', default=False,\n",
    "                    help='Do not use bias')\n",
    "parser.add_argument('--linear', action='store_true', default=False,\n",
    "                    help='Linear activation function')\n",
    "parser.add_argument('--sigmas', type=str, default=None,\n",
    "                    help='Sigmas')     \n",
    "parser.add_argument('-r','--s-range', nargs='*', type=float,\n",
    "                    help='Range for sigmas')\n",
    "parser.add_argument('-w','--scales', nargs='*', type=float,\n",
    "                    help='scale of the weights')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, nargs='*', metavar='LR',\n",
    "                    help='learning rate (default: 1e-4)')              \n",
    "parser.add_argument('--normalized', action='store_true', default=False,\n",
    "                    help='normalize sample norm across features')\n",
    "parser.add_argument('--risk-loss', type=str, default='MSE', metavar='LOSS',\n",
    "                    help='Loss for validation')\n",
    "parser.add_argument('--jacobian', action='store_true', default=False,\n",
    "                    help='compute the SVD of the jacobian of the network')\n",
    "parser.add_argument('--save-results', action='store_true', default=False,\n",
    "                    help='Save the results for plots')\n",
    "parser.add_argument('--coupled_noise', action='store_true', default=False,\n",
    "                    help='Couple noise in output to large eigenvalues.')\n",
    "parser.add_argument('--sigma_noise', nargs='*', type=float, default=0.0,\n",
    "                    help='Output noise.')\n",
    "parser.add_argument('--pcs', type=int, default=None, \n",
    "                    help='Number of PCs to use in data.')\n",
    "parser.add_argument('--transform-data', action='store_true', default=False, \n",
    "                    help='Use data in transformed space')\n",
    "parser.add_argument('--details', type=str, metavar='N',\n",
    "                    default='no_detail_given',\n",
    "                    help='details about the experimental setup')\n",
    "\n",
    "\n",
    "args = parser.parse_args(cli_args.split())\n",
    "\n",
    "# directories\n",
    "root = pathlib.Path(args.root) if args.root else pathlib.Path.cwd().parent\n",
    "\n",
    "current_date = str(datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "args.outpath = (pathlib.Path.cwd().parent / 'results' / 'two_layer_nn' /  current_date)\n",
    "\n",
    "if args.save_results:\n",
    "    args.outpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "if args.seed is not None:\n",
    "    random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = 1      # dimension of y\n",
    "\n",
    "# sample training set from the linear model\n",
    "lin_model = linear_model(args.dim, sigma_noise=args.sigma_noise, beta=None, normalized=False, sigmas=args.sigmas, s_range=args.s_range, coupled_noise=args.coupled_noise, transform_data=args.transform_data)\n",
    "Xs, ys = lin_model.sample(args.samples)\n",
    "Xs = torch.Tensor(Xs).to(device)\n",
    "ys = torch.Tensor(ys.reshape((-1,1))).to(device)\n",
    "\n",
    "if args.pcs:\n",
    "    Xs = prune_data(Xs, args.pcs)\n",
    "\n",
    "# sample the set for empirical risk calculation\n",
    "Xt, yt = lin_model.sample(args.samples)\n",
    "Xt = torch.Tensor(Xt).to(device)\n",
    "yt = torch.Tensor(yt.reshape((-1,1))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "risk_fn = torch.nn.L1Loss(reduction='mean') if args.risk_loss == 'L1' else loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jacobian_two_layer(X, y, model, crit):\n",
    "    \n",
    "    grads = []\n",
    "    for cx, cy in zip(X, y):\n",
    "\n",
    "        cur_grads = []\n",
    "        model.zero_grad()\n",
    "        co = model(cx)\n",
    "        co.backward(torch.ones(len(cy)))\n",
    "\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None and len(p.data.shape)>1:\n",
    "                cur_grads.append(p.grad.data.numpy().flatten())\n",
    "        grads.append(np.concatenate(cur_grads))\n",
    "    return np.array(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two layer neural network in pytorch\n",
    "model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(args.dim, args.hidden, bias=not args.no_bias),\n",
    "        torch.nn.Identity() if args.linear else torch.nn.ReLU(),\n",
    "        torch.nn.Linear(args.hidden, 1, bias=not args.no_bias),\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "#### re-initialize the weights (regular initialization is too unstable)\n",
    "# if args.scales:\n",
    "#     i = 0\n",
    "#     with torch.no_grad():\n",
    "#         for m in model:\n",
    "#             if type(m) == torch.nn.Linear:\n",
    "#                 if i == 0:\n",
    "#                     m.weight.data.normal_(0, args.scales[0])\n",
    "#                 if i == 1:\n",
    "#                     m.weight.data.uniform_(-args.scales[1], args.scales[1])\n",
    "#                 i += 1\n",
    "                \n",
    "                \n",
    "# use kaiming initialization instead                \n",
    "if args.scales:\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for m in model:\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                if i == 0:\n",
    "                    torch.nn.init.kaiming_normal_(m.weight, a=math.sqrt(5))\n",
    "                    m.weight.data = torch.mul(m.weight.data, args.scales[0])\n",
    "                if i == 1:\n",
    "                    torch.nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                    m.weight.data = torch.mul(m.weight.data, args.scales[1])\n",
    "                i += 1\n",
    "                \n",
    "\n",
    "# use same learning rate for the two layers in case of a single learning rate or none.\n",
    "if isinstance(args.lr, float):\n",
    "    args.lr = [args.lr] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the Jacobian at initialization\n",
    "if args.jacobian:\n",
    "    J = get_jacobian_two_layer(Xs, ys, model, loss_fn)\n",
    "    uv, sv, vtv = np.linalg.svd(J)\n",
    "\n",
    "    v1 = []\n",
    "    v2 = []\n",
    "    for i in range(sv.shape[0]):\n",
    "        v1.append(np.linalg.norm(vtv[i,:][:np.prod([250, 50])]))\n",
    "        v2.append(np.linalg.norm(vtv[i,:][-np.prod([1, 250]):]))\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    vTrec = np.linalg.norm(np.stack((v1, v2)), axis=0)\n",
    "\n",
    "\n",
    "    if args.save_results:\n",
    "        save_config(args)\n",
    "\n",
    "        np_save_file = args.outpath / ('two_layer_nn_jacobian_' + \n",
    "                                       str(args.scales[0]).replace('.', '-') + '_' + \n",
    "                                       str(args.scales[1]).replace('.', '-') + \n",
    "                                       '.txt')\n",
    "\n",
    "        np.savetxt(np_save_file, \n",
    "                   np.column_stack((sv, \n",
    "                                    v1,\n",
    "                                    v2,\n",
    "                                    vTrec\n",
    "                                   )), \n",
    "                   header='t vw vv vT', \n",
    "                   comments='',\n",
    "                   newline='\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "colorList = [cmap(50/1000), cmap(350/1000), cmap(650/1000)]\n",
    "labelList = [r'$W$', r'$v$', r'$W + v$']\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "ax_list = [plt.subplot(111)]\n",
    "\n",
    "ax_list[0].scatter(sv, v1, \n",
    "                color=colorList[0], \n",
    "                label=labelList[0],\n",
    "                lw=4)\n",
    "ax_list[0].scatter(sv, v2, \n",
    "                color=colorList[1], \n",
    "                label=labelList[1],\n",
    "#                 ls='dashed',\n",
    "                lw=4)\n",
    "ax_list[0].scatter(sv, vTrec, \n",
    "                color=colorList[2], \n",
    "                label=labelList[2],\n",
    "#                 ls='dashed',\n",
    "                lw=4)\n",
    "    \n",
    "ax_list[-1].legend(loc=0, bbox_to_anchor=(1, 0.5), fontsize='x-large',\n",
    "                   frameon=True, fancybox=True, shadow=True, ncol=1)\n",
    "ax_list[0].set_ylabel(r'$\\Vert v \\Vert_2^2$')\n",
    "\n",
    "# for i, ax in enumerate(ax_list): ax.set_title(r'$w = $' + str(weights[i]['w']) + \n",
    "#                                               r';$v = $' + str(weights[i]['v']))\n",
    "for ax in ax_list: ax.set_xlabel(r'$\\sigma_i$')\n",
    "for ax in ax_list: ax.set_xscale('log')\n",
    "for ax in ax_list: ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
