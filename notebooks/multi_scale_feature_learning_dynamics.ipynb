{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multi-scale feature learning dynamics\n",
    "Attempt to reproduce Figure 1 in \"Multi-scale-feature dynamics...\" and compare with empirical curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import matplotlib as mpl\n",
    "#mpl.use('tkagg')\n",
    "from scipy.stats import ortho_group\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some help functions, taken (and adapted) from https://github.com/mpezeshki/Epoch_wise_Double_Descent/tree/main\n",
    "\n",
    "def get_modulation_matrix(d, p, k):\n",
    "    U = ortho_group.rvs(d)\n",
    "    VT = ortho_group.rvs(d)\n",
    "    S = np.eye(d)\n",
    "    S[:p, :p] *= 1\n",
    "    S[p:, p:] *= 1 / k\n",
    "    F = np.dot(U, np.dot(S, VT))\n",
    "    return F\n",
    "\n",
    "\n",
    "# Implements the teacher and generates the data\n",
    "def get_data(seed, n, d, p, k, noise):\n",
    "    np.random.seed(seed)\n",
    "    Z = np.random.randn(n, d) / np.sqrt(d)\n",
    "    Z_test = np.random.randn(1000, d) / np.sqrt(d)\n",
    "\n",
    "    # teacher\n",
    "    w = np.random.randn(d, 1)\n",
    "    y = np.dot(Z, w)\n",
    "    y = y + noise * np.random.randn(*y.shape)\n",
    "    # test data is noiseless\n",
    "    y_test = np.dot(Z_test, w)\n",
    "\n",
    "    # the modulation matrix that controls students access to the data\n",
    "    F = get_modulation_matrix(d, p, k)\n",
    "\n",
    "    # X = F^T Z\n",
    "    X = np.dot(Z, F)\n",
    "    X_test = np.dot(Z_test, F)\n",
    "\n",
    "    return X, y, X_test, y_test, F, w\n",
    "\n",
    "\n",
    "def get_RQ_theoretical(L, Ft, FtInv, d, eta, l2, noise, t):\n",
    "    \n",
    "    # R: the alignment between the teacher and the student\n",
    "    \n",
    "    D = np.eye(d) - (np.eye(d) - eta * L)**t\n",
    "    D2 = np.dot(L,  np.linalg.inv(L + l2 * np.eye(d)))\n",
    "    R = np.trace(np.dot(D, D2)) / d\n",
    "    \n",
    "    # Q: the student's modulated norm\n",
    "    D3 = np.dot(np.dot(Ft, D), D2)\n",
    "    A = np.dot(D3, FtInv)\n",
    "    B = np.dot(D3, np.linalg.inv(L**0.5))\n",
    "    Q = (np.trace(np.dot(A.T, A)) + noise * np.trace(np.dot(B.T, B))) / d\n",
    "    \n",
    "    return R, Q\n",
    "\n",
    "def get_RQ_empirical(w_hat, F, w, d):\n",
    "    # R: the alignment between the teacher and the student\n",
    "    R = np.dot(np.dot(F, w_hat).T, w).item() / d\n",
    "    # Q: the student's modulated norm\n",
    "    Q = np.dot(np.dot(F, w_hat).T, np.dot(F, w_hat)).item() / d\n",
    "    return R, Q\n",
    "\n",
    "def get_w_hat_gd(X, y, l2):\n",
    "    return np.linalg.inv(np.dot(X.T, X) + l2 * np.eye(X.shape[-1])) @ X.T @ y\n",
    "\n",
    "def training_step(X, w_hat, w_hat_gd, eta, l2, t):\n",
    "    d = w_hat.shape[0]\n",
    "    return (np.eye(d) - ((1-eta * l2) * np.eye(d) - eta * np.dot(X.T, X))**t) @ w_hat_gd\n",
    "\n",
    "\n",
    "def get_loss_theoretical(R, Q, noise):\n",
    "    return 0.5 * (1 + noise + Q - 2*R)\n",
    "\n",
    "def get_loss_empirical(R, Q):\n",
    "    return 0.5 * (1 + Q - 2*R) # Or should we account for the label noise somehow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n: number of training examples\n",
    "n = 130\n",
    "# d: number of total dimensions\n",
    "d = 100\n",
    "# p: number of fast learning dimensions\n",
    "p = 70\n",
    "# k: kappa -> the condition number of the modulation matrix, F\n",
    "k = 100\n",
    "# standard deviation of the noise added to the teacher output\n",
    "noise = 0.0\n",
    "# L2 regularization coefficient\n",
    "l2 = 0.0\n",
    "# eta: learning rate\n",
    "eta = 0.1\n",
    "# T: Num training iterations\n",
    "T = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical curves\n",
    "def get_theoretical_results(X, F, d, eta, l2, noise, T):\n",
    "\n",
    "    # Some help vars\n",
    "    _, S, Vh = np.linalg.svd(F, full_matrices=True) # X.T@X = F.T@F=VSV.T in expectation (but not sure all expressions pan out)\n",
    "    # It also makes a very big difference whether I use X or F, so...\n",
    "    \n",
    "    if S.shape[0] == d: # Full rank (which we kind of expect it to have)\n",
    "        L = np.diag(S)\n",
    "    else:\n",
    "        L = np.diag(np.array([S, np.zeros((d - S.shape[0]))]))\n",
    "    \n",
    "    assert L.shape == (d, d) # TODO: Vad händer om d > n?\n",
    "    \n",
    "    Ft = np.dot(F, Vh.T)\n",
    "    FtInv = np.linalg.inv(Ft)\n",
    "    \n",
    "    loss_gen = np.zeros((T,))\n",
    "    for t in range(T):\n",
    "        R, Q = get_RQ_theoretical(L, Ft, FtInv, d, eta, l2, noise, t)\n",
    "        loss_gen[t] = get_loss_theoretical(R, Q, noise)\n",
    "       \n",
    "    return loss_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = range(10)\n",
    "\n",
    "loss_gen, loss_gen_fast, loss_gen_slow = np.zeros((T,)), np.zeros((T,)), np.zeros((T,))\n",
    "for seed in seeds:\n",
    "    X, _, _, _, F, _ = get_data(seed, n, d, p, k, noise)\n",
    "    loss_gen += get_theoretical_results(X, F, d, eta, l2, noise, T) / len(seeds)\n",
    "    \n",
    "    # Train fast features only\n",
    "    loss_gen_fast += get_theoretical_results(X[:, :p], F[:p, :p], p, eta, l2, noise, T) / len(seeds)\n",
    "    \n",
    "    # Train slow features only\n",
    "    loss_gen_slow += get_theoretical_results(X[:, p:], F[p:, p:], d-p, eta, l2, noise, T) / len(seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xscale('log')\n",
    "ax.plot(loss_gen[1:], label=\"Both\")\n",
    "ax.plot(loss_gen_fast[1:], label=\"Fast\")\n",
    "ax.plot(loss_gen_slow[1:], label=\"Slow\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Its.\")\n",
    "ax.set_ylabel(\"Generalisation loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical curves\n",
    "def get_empirical_results(X, y, F, w, d, eta, l2, noise, T):\n",
    "\n",
    "    # Some help vars\n",
    "    w_hat_gd = get_w_hat_gd(X, y, l2)\n",
    "\n",
    "    w_hat = np.zeros((d, 1))\n",
    "    loss_gen = np.zeros((T,))\n",
    "    for t in range(T):\n",
    "        w_hat = training_step(X, w_hat, w_hat_gd, eta, l2, t)\n",
    "        R, Q = get_RQ_empirical(w_hat, F, w, d)\n",
    "        loss_gen[t] = get_loss_theoretical(R, Q, noise)\n",
    "       \n",
    "    return loss_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = range(10)\n",
    "\n",
    "# TODO: Jag vet inte hur de menar att de tränar bara fast/slow features; det kan vara så här \n",
    "# (men jag verkar få dd för fast features bara, kan vara att detta är empiriskt)\n",
    "loss_gen, loss_gen_fast, loss_gen_slow = np.zeros((T,)), np.zeros((T,)), np.zeros((T,))\n",
    "for seed in seeds:\n",
    "    # This is still when we have taken expectation w.r.t. z already (so perhaps not entirely correct to call it empirical)\n",
    "    X, y, X_test, y_test, F, w = get_data(seed, n, d, p, k, noise)\n",
    "    \n",
    "    # Train all features\n",
    "    loss_gen += get_empirical_results(X, y, F, w, d, eta, l2, noise, T) / len(seeds) # How am I supposed to get an estimate of the noise, without it being equal to the MSE??\n",
    "    \n",
    "    # Train fast features only\n",
    "    #eta_d = np.zeros((d,))\n",
    "    #eta_d[:p] = eta\n",
    "    loss_gen_fast += get_empirical_results(X[:, :p], y, F[:p, :p], w[:p], p, eta, l2, noise, T) / len(seeds)\n",
    "    \n",
    "    # Train slow features only\n",
    "    #eta_d = np.zeros((d,))\n",
    "    #eta_d[p:] = eta\n",
    "    loss_gen_slow += get_empirical_results(X[:, p:], y, F[p:, p:], w[p:], d-p, eta, l2, noise, T) / len(seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].plot(loss_gen[1:], label=\"Both\")\n",
    "ax[1].plot(loss_gen_fast[1:], label=\"Fast\")\n",
    "ax[1].plot(loss_gen_slow[1:], label=\"Slow\")\n",
    "\n",
    "for k in range(2):\n",
    "    ax[k].set_xscale('log')\n",
    "    ax[k].legend()\n",
    "    ax[k].set_xlabel(\"Its.\")\n",
    "    ax[k].set_ylabel(\"Generalisation loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_loss_surface(seed, k, noise, w_min=-10.0, w_max=10.0, w_step=0.1): \n",
    "    d, p, n = 2, 1, 1 # We do this only in two dimensions\n",
    "    _, _, _, _, F, w = get_data(seed, n, d, p, k, noise)\n",
    "\n",
    "    u, v = np.arange(w_min, w_max, w_step), np.arange(w_min, w_max, w_step)\n",
    "    U, V = np.meshgrid(u, v)\n",
    "    U, V = U.astype(np.float32), V.astype(np.float32)\n",
    "\n",
    "    nu, nv = u.shape[0], v.shape[0]\n",
    "    losses = np.zeros((nv, nu))\n",
    "    for i in range(nu):\n",
    "        for j in range(nv):\n",
    "            # Set model weights\n",
    "            w_hat = np.array([U[j, i], V[j, i]]).reshape(-1, 1)\n",
    "\n",
    "            # Calculate loss\n",
    "            R, Q = get_RQ_empirical(w_hat, F, w, d)\n",
    "            losses[j, i] = get_loss_theoretical(R, Q, noise)\n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    im1 = ax.imshow(losses)\n",
    "    ax.plot()\n",
    "    fig.colorbar(im1, ax=ax)\n",
    "\n",
    "    tick_step = int(nu / 4)\n",
    "    ax.set_xticks([i for i in range(nu + 1) if np.mod(i, tick_step) == 0])\n",
    "    ax.set_xticklabels([w_min + i * (w_max - w_min) / nu for i in range(nu + 1) if np.mod(i, tick_step) == 0])\n",
    "    ax.set_xlabel(\"u\")\n",
    "\n",
    "    ax.set_yticks([i for i in range(nv + 1) if np.mod(i, tick_step) == 0])\n",
    "    ax.set_yticklabels([w_min + i * (w_max - w_min) / nu for i in range(nv + 1) if np.mod(i, tick_step) == 0])\n",
    "    ax.set_ylabel(\"v\")\n",
    "    \n",
    "    plt.show()\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = plot_loss_surface(12, k, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
